<!DOCTYPE html>
<html lang="de">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.3.0">
  <meta name="generator" content="Hugo 0.55.0" />
  <meta name="author" content="Patrick Christian Trettenbrein">

  
  
  
  
    
  
  <meta name="description" content="&lt;p&gt;&lt;b&gt;Background&lt;/b&gt;&lt;/p&gt; &lt;p&gt;The ability to combine individual lexical items into phrases and sentences is at the core of the human capacity for language (Friederici et al., 2017). Linguistic research indicates that the world’s sign languages exhibit complex hierarchical organisation of utterances just like spoken languages (e.g., Cecchetto, 2017), but the role of hierarchical organisation during online sign language processing is poorly understood. The present study constitutes the first adaptation of the classical psycholinguistic “click” paradigm (e.g., Holmes &amp; Forster, 1970) from the auditory-oral to the visuo-spatial modality. Using short flashes inserted into videos of signed sentences as analogues to auditory clicks, we seek to determine whether deaf signers, like hearing speakers, automatically attribute constituent structure onto sequences of signs during language comprehension.&lt;/p&gt; &lt;p&gt;&lt;b&gt;Methods&lt;/b&gt;&lt;/p&gt; &lt;p&gt;The paradigm is implemented as an automated reaction-time experiment which can comfortably be run by deaf participants from home via their web browser. Instructions are given in German Sign Language (DGS) in the form of pre-recorded videos. In the experiment, participants watch different types of complex DGS sentences such as (1).
&lt;p&gt;(1) IF POSS&lt;sub&gt;1&lt;/sub&gt; SISTER WITH POSS&lt;sub&gt;3&lt;/sub&gt; CHILD&#43;&#43; TOMORROW MORNING &lt;sub&gt;3&lt;/sub&gt;VISIT&lt;sub&gt;1&lt;/sub&gt; / IX&lt;sub&gt;1&lt;/sub&gt; HAVE-TO HOUSE CLEAN&lt;/p&gt;
During the presentation of sentences, a white flash (duration: 80 ms) may occur as an overlay to the stimulus clip at different positions in the sentence and participants have to respond to this cue as fast as possible via button press. After every trial, participants have to answer a binary comprehension question (Figure 1). The flash can occur in the first or second half of the sentence. Importantly, the exact point in time when the flash occurs differs with regard to the syntactic structure of a sentence, so that a flash may occur either after a major break in the constituent structure separating two clauses as indicated by “/” in (1), after a minor break, or not at a break. This yields a 2x3 within-subject design with the factors &lt;i&gt;Position&lt;/i&gt; (first vs. second half) and &lt;i&gt;Structure&lt;/i&gt; (major vs. minor vs. no break). In addition to the six experimental conditions, filler trials (22 %) in which no flash occurs were also included. The stimuli were designed and recorded with a deaf native signer. All clips were annotated using ELAN (Lausberg &amp; Sloeties, 2009) and flashes were inserted using an automated video-editing procedure. In addition, we performed automated motion-tracking on the stimuli using OpenPose (Cao et al., 2019) and extracted motion information using OpenPoseR (Trettenbrein &amp; Zaccarella, 2021) to control our stimuli for a possible correlation between articulatory pauses and the probed constituent structure.&lt;/p&gt; &lt;p&gt;&lt;b&gt;Discussion&lt;/b&gt;&lt;/p&gt; &lt;p&gt;At the time of writing, data collection is still ongoing which is why we will limit our discussion here to the effects we expect to observe. Assuming that the placement of flashes at different positions in the constituent structure of sentences will impact the time that participants take to respond, we expect to observe a main effect of &lt;i&gt;Structure&lt;/i&gt;. In particular, faster RTs are expected for detecting a flash at a major (no constituent interrupted) and minor (small number of constituents interrupted) boundaries, compared to the no boundary condition (large number of constituents interrupted). This would provide first psycholinguistic evidence for the relevance of constituent structure during sign language comprehension, expanding previous findings for spoken language. We do not expect to observe a main effect of &lt;i&gt;Position&lt;/i&gt;, due to the inclusion of filler trials without any flashes which should counteract the increased probability of requiring a response towards the second half of the sentence, which was inherent to the design of earlier auditory studies (Holmes &amp; Forster, 1970). In sum, the expected effect of &lt;i&gt;Structure&lt;/i&gt; would provide evidence for the modality-independence of the cognitive mechanisms underlying syntactic processing.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#39;mso-no-proof:yes&#39;&gt;&lt;img width=100% src=&#39;https://i.imgur.com/XOMG6oh.png&#39;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=MsoCaption style=&#39;page-break-after:avoid&#39;&gt;&lt;small&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;Figure &lt;/span&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;&lt;span style=&#39;mso-no-proof: yes&#39;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;: Example of an experimental trial in which the sentence given in (1) is presented and a flash occurs after the sign VISIT at the major break separating the two clauses. Every trial is followed by a comprehension question.&lt;/span&gt;&lt;span style=&#39;font-family:&#39;Times New Roman&#39;,serif;mso-ansi-language: EN-US&#39;&gt;&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;References&lt;/b&gt;&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., &amp; Sheikh, Y. (2019). OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. &lt;i&gt;ArXiv&lt;/i&gt;:1812.08008 [Cs]. http://arxiv.org/abs/1812.08008&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Cecchetto, C. (2017). The syntax of sign language and Universal Grammar. In I. Roberts (Ed.), &lt;i&gt;The Oxford handbook of Universal Grammar&lt;/i&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Friederici, A. D., Chomsky, N., Berwick, R. C., Moro, A., &amp; Bolhuis, J. J. (2017). Language, mind and brain. &lt;i&gt;Nature Human Behaviour&lt;/i&gt;. 1, 713–722. https://dx.doi.org/10.1038/s41562-017-0184-4 &lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Holmes, V. M., &amp; Forster, K. I. (1970). Detection of extraneous signals during sentence recognition. &lt;i&gt;Perception &amp; Psychophysics&lt;/i&gt;, 7(5), 297–301.&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Lausberg, H., &amp; Sloetjes, H. (2009). Coding gestural behavior with the NEUROGES-ELAN system. &lt;i&gt;Behavior Research Methods&lt;/i&gt;, 41(3), 841–849. &lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Trettenbrein, P. C., &amp; Zaccarella, E. (2021). Controlling video stimuli in sign language and gesture research: The OpenPoseR package for analyzing OpenPose motion-tracking data in R. &lt;i&gt;Frontiers in Psychology&lt;/i&gt;, 12, 628728. https://dx.doi.org/10.3389/fpsyg.2021.628728&lt;/p&gt; ">

  
  <link rel="alternate" hreflang="en" href="https://trettenbrein.biolinguistics.eu/talk/can-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr/">
  
  <link rel="alternate" hreflang="de" href="https://trettenbrein.biolinguistics.eu/de/talk/can-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#1185C1">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/de/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://trettenbrein.biolinguistics.eu/index.xml" type="application/rss+xml" title="Patrick C. Trettenbrein">
  <link rel="feed" href="https://trettenbrein.biolinguistics.eu/index.xml" type="application/rss+xml" title="Patrick C. Trettenbrein">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://trettenbrein.biolinguistics.eu/de/talk/can-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@ptrettenbrein">
  <meta property="twitter:creator" content="@ptrettenbrein">
  
  <meta property="og:site_name" content="Patrick C. Trettenbrein">
  <meta property="og:url" content="https://trettenbrein.biolinguistics.eu/de/talk/can-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr/">
  <meta property="og:title" content="Can detection of extraneous visual signals reveal the syntactic structure of sign language? | Patrick C. Trettenbrein">
  <meta property="og:description" content="&lt;p&gt;&lt;b&gt;Background&lt;/b&gt;&lt;/p&gt; &lt;p&gt;The ability to combine individual lexical items into phrases and sentences is at the core of the human capacity for language (Friederici et al., 2017). Linguistic research indicates that the world’s sign languages exhibit complex hierarchical organisation of utterances just like spoken languages (e.g., Cecchetto, 2017), but the role of hierarchical organisation during online sign language processing is poorly understood. The present study constitutes the first adaptation of the classical psycholinguistic “click” paradigm (e.g., Holmes &amp; Forster, 1970) from the auditory-oral to the visuo-spatial modality. Using short flashes inserted into videos of signed sentences as analogues to auditory clicks, we seek to determine whether deaf signers, like hearing speakers, automatically attribute constituent structure onto sequences of signs during language comprehension.&lt;/p&gt; &lt;p&gt;&lt;b&gt;Methods&lt;/b&gt;&lt;/p&gt; &lt;p&gt;The paradigm is implemented as an automated reaction-time experiment which can comfortably be run by deaf participants from home via their web browser. Instructions are given in German Sign Language (DGS) in the form of pre-recorded videos. In the experiment, participants watch different types of complex DGS sentences such as (1).
&lt;p&gt;(1) IF POSS&lt;sub&gt;1&lt;/sub&gt; SISTER WITH POSS&lt;sub&gt;3&lt;/sub&gt; CHILD&#43;&#43; TOMORROW MORNING &lt;sub&gt;3&lt;/sub&gt;VISIT&lt;sub&gt;1&lt;/sub&gt; / IX&lt;sub&gt;1&lt;/sub&gt; HAVE-TO HOUSE CLEAN&lt;/p&gt;
During the presentation of sentences, a white flash (duration: 80 ms) may occur as an overlay to the stimulus clip at different positions in the sentence and participants have to respond to this cue as fast as possible via button press. After every trial, participants have to answer a binary comprehension question (Figure 1). The flash can occur in the first or second half of the sentence. Importantly, the exact point in time when the flash occurs differs with regard to the syntactic structure of a sentence, so that a flash may occur either after a major break in the constituent structure separating two clauses as indicated by “/” in (1), after a minor break, or not at a break. This yields a 2x3 within-subject design with the factors &lt;i&gt;Position&lt;/i&gt; (first vs. second half) and &lt;i&gt;Structure&lt;/i&gt; (major vs. minor vs. no break). In addition to the six experimental conditions, filler trials (22 %) in which no flash occurs were also included. The stimuli were designed and recorded with a deaf native signer. All clips were annotated using ELAN (Lausberg &amp; Sloeties, 2009) and flashes were inserted using an automated video-editing procedure. In addition, we performed automated motion-tracking on the stimuli using OpenPose (Cao et al., 2019) and extracted motion information using OpenPoseR (Trettenbrein &amp; Zaccarella, 2021) to control our stimuli for a possible correlation between articulatory pauses and the probed constituent structure.&lt;/p&gt; &lt;p&gt;&lt;b&gt;Discussion&lt;/b&gt;&lt;/p&gt; &lt;p&gt;At the time of writing, data collection is still ongoing which is why we will limit our discussion here to the effects we expect to observe. Assuming that the placement of flashes at different positions in the constituent structure of sentences will impact the time that participants take to respond, we expect to observe a main effect of &lt;i&gt;Structure&lt;/i&gt;. In particular, faster RTs are expected for detecting a flash at a major (no constituent interrupted) and minor (small number of constituents interrupted) boundaries, compared to the no boundary condition (large number of constituents interrupted). This would provide first psycholinguistic evidence for the relevance of constituent structure during sign language comprehension, expanding previous findings for spoken language. We do not expect to observe a main effect of &lt;i&gt;Position&lt;/i&gt;, due to the inclusion of filler trials without any flashes which should counteract the increased probability of requiring a response towards the second half of the sentence, which was inherent to the design of earlier auditory studies (Holmes &amp; Forster, 1970). In sum, the expected effect of &lt;i&gt;Structure&lt;/i&gt; would provide evidence for the modality-independence of the cognitive mechanisms underlying syntactic processing.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#39;mso-no-proof:yes&#39;&gt;&lt;img width=100% src=&#39;https://i.imgur.com/XOMG6oh.png&#39;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=MsoCaption style=&#39;page-break-after:avoid&#39;&gt;&lt;small&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;Figure &lt;/span&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;&lt;span style=&#39;mso-no-proof: yes&#39;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;span lang=EN-GB style=&#39;font-family:&#39;Times New Roman&#39;,serif&#39;&gt;: Example of an experimental trial in which the sentence given in (1) is presented and a flash occurs after the sign VISIT at the major break separating the two clauses. Every trial is followed by a comprehension question.&lt;/span&gt;&lt;span style=&#39;font-family:&#39;Times New Roman&#39;,serif;mso-ansi-language: EN-US&#39;&gt;&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;References&lt;/b&gt;&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., &amp; Sheikh, Y. (2019). OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. &lt;i&gt;ArXiv&lt;/i&gt;:1812.08008 [Cs]. http://arxiv.org/abs/1812.08008&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Cecchetto, C. (2017). The syntax of sign language and Universal Grammar. In I. Roberts (Ed.), &lt;i&gt;The Oxford handbook of Universal Grammar&lt;/i&gt;. Oxford: Oxford University Press.&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Friederici, A. D., Chomsky, N., Berwick, R. C., Moro, A., &amp; Bolhuis, J. J. (2017). Language, mind and brain. &lt;i&gt;Nature Human Behaviour&lt;/i&gt;. 1, 713–722. https://dx.doi.org/10.1038/s41562-017-0184-4 &lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Holmes, V. M., &amp; Forster, K. I. (1970). Detection of extraneous signals during sentence recognition. &lt;i&gt;Perception &amp; Psychophysics&lt;/i&gt;, 7(5), 297–301.&lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Lausberg, H., &amp; Sloetjes, H. (2009). Coding gestural behavior with the NEUROGES-ELAN system. &lt;i&gt;Behavior Research Methods&lt;/i&gt;, 41(3), 841–849. &lt;/p&gt;
&lt;p style=&#39;margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm&#39;&gt;Trettenbrein, P. C., &amp; Zaccarella, E. (2021). Controlling video stimuli in sign language and gesture research: The OpenPoseR package for analyzing OpenPose motion-tracking data in R. &lt;i&gt;Frontiers in Psychology&lt;/i&gt;, 12, 628728. https://dx.doi.org/10.3389/fpsyg.2021.628728&lt;/p&gt; "><meta property="og:image" content="https://trettenbrein.biolinguistics.eu/img/portrait.jpg">
  <meta property="og:locale" content="de">
  
  <meta property="article:published_time" content="2022-06-28T00:00:00&#43;01:00">
  
  <meta property="article:modified_time" content="2022-06-28T00:00:00&#43;01:00">
  

  

  

  <title>Can detection of extraneous visual signals reveal the syntactic structure of sign language? | Patrick C. Trettenbrein</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Suche</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Suche..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/de/">Patrick C. Trettenbrein</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Zur Navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/de/#about">
            
            <span>Startseite</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/de/#publications">
            
            <span>Publikationen</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/de/#talks">
            
            <span>Vorträge</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/de/#contact">
            
            <span>Kontakt</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true">
            <i class="fas fa-globe" aria-hidden="true"></i>
            <span>Deutsch</span>
          </a>
          <ul class="dropdown-menu">
            
            <li class="dropdown-item my-0 py-0 mx-0 px-0">
              <a href="https://trettenbrein.biolinguistics.eu/talk/can-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr/">
                <span>English</span>
              </a>
            </li>
            
          </ul>
        </li>
        

        

      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/Event">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Can detection of extraneous visual signals reveal the syntactic structure of sign language?</h1>

  

  
    

<div class="article-metadata">

  
  
  
  <div>
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">PC Trettenbrein</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">M Maran</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">N-K Pendzich</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">J Pohl</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">T Finkbeiner</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">M Steinbach</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">AD Friederici</span>
    </span>, 
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">E Zaccarella</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2022-06-28 00:00:00 &#43;0100 &#43;0100" itemprop="datePublished">
    <time datetime="2022-06-28 00:00:00 &#43;0100 &#43;0100" itemprop="dateModified">
      Jun 28, 2022
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Patrick Christian Trettenbrein">
  </span>

  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Can%20detection%20of%20extraneous%20visual%20signals%20reveal%20the%20syntactic%20structure%20of%20sign%20language%3f&amp;url=https%3a%2f%2ftrettenbrein.biolinguistics.eu%2fde%2ftalk%2fcan-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2ftrettenbrein.biolinguistics.eu%2fde%2ftalk%2fcan-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrettenbrein.biolinguistics.eu%2fde%2ftalk%2fcan-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr%2f&amp;title=Can%20detection%20of%20extraneous%20visual%20signals%20reveal%20the%20syntactic%20structure%20of%20sign%20language%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2ftrettenbrein.biolinguistics.eu%2fde%2ftalk%2fcan-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr%2f&amp;title=Can%20detection%20of%20extraneous%20visual%20signals%20reveal%20the%20syntactic%20structure%20of%20sign%20language%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Can%20detection%20of%20extraneous%20visual%20signals%20reveal%20the%20syntactic%20structure%20of%20sign%20language%3f&amp;body=https%3a%2f%2ftrettenbrein.biolinguistics.eu%2fde%2ftalk%2fcan-detection-of-extraneous-visual-signals-reveal-the-syntactic-structure-of-sign-language-tislr%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    







  








<div class="btn-links mb-3">
  
  







  
    
  


<a class="btn btn-outline-primary my-1 mr-1" href="https://www.dropbox.com/s/bnhaypmehmlwppl/TISLR14_Abstract_4.pdf?dl=0" target="_blank" rel="noopener">
  PDF
</a>

















</div>


  
</div>



  <div class="article-container">

    
      <h3>Zusammenfassung</h3>
      <p class="pub-abstract" itemprop="text"><p><b>Background</b></p> <p>The ability to combine individual lexical items into phrases and sentences is at the core of the human capacity for language (Friederici et al., 2017). Linguistic research indicates that the world’s sign languages exhibit complex hierarchical organisation of utterances just like spoken languages (e.g., Cecchetto, 2017), but the role of hierarchical organisation during online sign language processing is poorly understood. The present study constitutes the first adaptation of the classical psycholinguistic “click” paradigm (e.g., Holmes & Forster, 1970) from the auditory-oral to the visuo-spatial modality. Using short flashes inserted into videos of signed sentences as analogues to auditory clicks, we seek to determine whether deaf signers, like hearing speakers, automatically attribute constituent structure onto sequences of signs during language comprehension.</p> <p><b>Methods</b></p> <p>The paradigm is implemented as an automated reaction-time experiment which can comfortably be run by deaf participants from home via their web browser. Instructions are given in German Sign Language (DGS) in the form of pre-recorded videos. In the experiment, participants watch different types of complex DGS sentences such as (1).
<p>(1) IF POSS<sub>1</sub> SISTER WITH POSS<sub>3</sub> CHILD++ TOMORROW MORNING <sub>3</sub>VISIT<sub>1</sub> / IX<sub>1</sub> HAVE-TO HOUSE CLEAN</p>
During the presentation of sentences, a white flash (duration: 80 ms) may occur as an overlay to the stimulus clip at different positions in the sentence and participants have to respond to this cue as fast as possible via button press. After every trial, participants have to answer a binary comprehension question (Figure 1). The flash can occur in the first or second half of the sentence. Importantly, the exact point in time when the flash occurs differs with regard to the syntactic structure of a sentence, so that a flash may occur either after a major break in the constituent structure separating two clauses as indicated by “/” in (1), after a minor break, or not at a break. This yields a 2x3 within-subject design with the factors <i>Position</i> (first vs. second half) and <i>Structure</i> (major vs. minor vs. no break). In addition to the six experimental conditions, filler trials (22 %) in which no flash occurs were also included. The stimuli were designed and recorded with a deaf native signer. All clips were annotated using ELAN (Lausberg & Sloeties, 2009) and flashes were inserted using an automated video-editing procedure. In addition, we performed automated motion-tracking on the stimuli using OpenPose (Cao et al., 2019) and extracted motion information using OpenPoseR (Trettenbrein & Zaccarella, 2021) to control our stimuli for a possible correlation between articulatory pauses and the probed constituent structure.</p> <p><b>Discussion</b></p> <p>At the time of writing, data collection is still ongoing which is why we will limit our discussion here to the effects we expect to observe. Assuming that the placement of flashes at different positions in the constituent structure of sentences will impact the time that participants take to respond, we expect to observe a main effect of <i>Structure</i>. In particular, faster RTs are expected for detecting a flash at a major (no constituent interrupted) and minor (small number of constituents interrupted) boundaries, compared to the no boundary condition (large number of constituents interrupted). This would provide first psycholinguistic evidence for the relevance of constituent structure during sign language comprehension, expanding previous findings for spoken language. We do not expect to observe a main effect of <i>Position</i>, due to the inclusion of filler trials without any flashes which should counteract the increased probability of requiring a response towards the second half of the sentence, which was inherent to the design of earlier auditory studies (Holmes & Forster, 1970). In sum, the expected effect of <i>Structure</i> would provide evidence for the modality-independence of the cognitive mechanisms underlying syntactic processing.</p>
<p><span style='mso-no-proof:yes'><img width=100% src='https://i.imgur.com/XOMG6oh.png'></span></p>
<p class=MsoCaption style='page-break-after:avoid'><small><span lang=EN-GB style='font-family:'Times New Roman',serif'>Figure </span><span lang=EN-GB style='font-family:'Times New Roman',serif'><span style='mso-no-proof: yes'>1</span></span><span lang=EN-GB style='font-family:'Times New Roman',serif'>: Example of an experimental trial in which the sentence given in (1) is presented and a flash occurs after the sign VISIT at the major break separating the two clauses. Every trial is followed by a comprehension question.</span><span style='font-family:'Times New Roman',serif;mso-ansi-language: EN-US'></span></small></p>
<p><b>References</b></p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., & Sheikh, Y. (2019). OpenPose: Realtime multi-person 2D pose estimation using part affinity fields. <i>ArXiv</i>:1812.08008 [Cs]. http://arxiv.org/abs/1812.08008</p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Cecchetto, C. (2017). The syntax of sign language and Universal Grammar. In I. Roberts (Ed.), <i>The Oxford handbook of Universal Grammar</i>. Oxford: Oxford University Press.</p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Friederici, A. D., Chomsky, N., Berwick, R. C., Moro, A., & Bolhuis, J. J. (2017). Language, mind and brain. <i>Nature Human Behaviour</i>. 1, 713–722. https://dx.doi.org/10.1038/s41562-017-0184-4 </p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Holmes, V. M., & Forster, K. I. (1970). Detection of extraneous signals during sentence recognition. <i>Perception & Psychophysics</i>, 7(5), 297–301.</p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Lausberg, H., & Sloetjes, H. (2009). Coding gestural behavior with the NEUROGES-ELAN system. <i>Behavior Research Methods</i>, 41(3), 841–849. </p>
<p style='margin-top:0cm;margin-right:0cm;margin-bottom:0cm;margin-left:1.0cm; margin-bottom:.0001pt;text-indent:-1.0cm'>Trettenbrein, P. C., & Zaccarella, E. (2021). Controlling video stimuli in sign language and gesture research: The OpenPoseR package for analyzing OpenPose motion-tracking data in R. <i>Frontiers in Psychology</i>, 12, 628728. https://dx.doi.org/10.3389/fpsyg.2021.628728</p> 
</p>
    

    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Datum</div>
          <div class="col-12 col-md-9" itemprop="datePublished">
            
            Sep 27, 2022
            <div class="talk-time">
              
                5:30 PM
                
                  &mdash; 7:00 PM
                
              
            </div>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>

    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Veranstaltung</div>
          <div class="col-12 col-md-9">
            <a href="https://tislr2022.jp" target="_blank" rel="noopener">
            Theoretical Issues in Sign Language Research (TISLR) 14
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Ort</div>
          <div class="col-12 col-md-9">Osaka, Japan</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style">
      
    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/de/tags/talk/">talk</a>
  
  <a class="badge badge-light" href="">sign language</a>
  
  <a class="badge badge-light" href="/de/tags/syntax/">syntax</a>
  
  <a class="badge badge-light" href="">constituent structure</a>
  
  <a class="badge badge-light" href="/de/tags/psycholinguistics/">psycholinguistics</a>
  
</div>



    



  







  </div>
</div>



<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018&ndash;2019 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Zitieren</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Kopie
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/de/index.json";
      const i18n = {
        'placeholder': "Suche...",
        'results': "Suchergebnisse",
        'no_results': "Nichts gefunden"
      };
      const content_type = {
        'post': "Blog",
        'project': "Projekte",
        'publication' : "Publikationen",
        'talk' : "Vorträge"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    <script src="/js/academic.min.07fbebbbf71b021c8836e1d7ecffa489.js"></script>

    

  </body>
</html>

